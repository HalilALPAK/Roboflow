{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "852c8cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\user\\anaconda3\\lib\\site-packages (8.3.107)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ultralytics) (2.1.1)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ultralytics) (3.10.6)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ultralytics) (11.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ultralytics) (2.5.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ultralytics) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\anaconda3\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\user\\anaconda3\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.10.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\User\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\User\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\User\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\User\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\User\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\User\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901315eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: basket_best.pt\n",
      "Video: basket_test1.mp4\n",
      "Loading model...\n",
      "Model loaded successfully!\n",
      "Video properties:\n",
      "  Size: 576x1024\n",
      "  FPS: 30\n",
      "  Total frames: 465\n",
      "Processing video...\n",
      "Processed: 10/465 frames (2.2%)\n",
      "Processed: 20/465 frames (4.3%)\n",
      "Processed: 30/465 frames (6.5%)\n",
      "Processed: 40/465 frames (8.6%)\n",
      "Processed: 50/465 frames (10.8%)\n",
      "Processed: 60/465 frames (12.9%)\n",
      "Processed: 70/465 frames (15.1%)\n",
      "Processed: 80/465 frames (17.2%)\n",
      "Processed: 90/465 frames (19.4%)\n",
      "Processed: 100/465 frames (21.5%)\n",
      "Processed: 110/465 frames (23.7%)\n",
      "Processed: 120/465 frames (25.8%)\n",
      "Processed: 130/465 frames (28.0%)\n",
      "Processed: 140/465 frames (30.1%)\n",
      "Processed: 150/465 frames (32.3%)\n",
      "Processed: 160/465 frames (34.4%)\n",
      "Processed: 170/465 frames (36.6%)\n",
      "Processed: 180/465 frames (38.7%)\n",
      "Processed: 190/465 frames (40.9%)\n",
      "Processed: 200/465 frames (43.0%)\n",
      "Processed: 210/465 frames (45.2%)\n",
      "Processed: 220/465 frames (47.3%)\n",
      "Processed: 230/465 frames (49.5%)\n",
      "Processed: 240/465 frames (51.6%)\n",
      "Processed: 250/465 frames (53.8%)\n",
      "Processed: 260/465 frames (55.9%)\n",
      "Processed: 270/465 frames (58.1%)\n",
      "Processed: 280/465 frames (60.2%)\n",
      "Processed: 290/465 frames (62.4%)\n",
      "Processed: 300/465 frames (64.5%)\n",
      "Processed: 310/465 frames (66.7%)\n",
      "Processed: 320/465 frames (68.8%)\n",
      "Processed: 330/465 frames (71.0%)\n",
      "Processed: 340/465 frames (73.1%)\n",
      "Processed: 350/465 frames (75.3%)\n",
      "Processed: 360/465 frames (77.4%)\n",
      "Processed: 370/465 frames (79.6%)\n",
      "Processed: 380/465 frames (81.7%)\n",
      "Processed: 390/465 frames (83.9%)\n",
      "Processed: 400/465 frames (86.0%)\n",
      "Processed: 410/465 frames (88.2%)\n",
      "Processed: 420/465 frames (90.3%)\n",
      "Processed: 430/465 frames (92.5%)\n",
      "Processed: 440/465 frames (94.6%)\n",
      "Processed: 450/465 frames (96.8%)\n",
      "Processed: 460/465 frames (98.9%)\n",
      "\n",
      "Video processing finished!\n",
      "Result video: basket_sonuc.mp4\n",
      "\n",
      "Completed! To open the result video: basket_sonuc.mp4\n"
     ]
    }
   ],
   "source": [
    "# Video Tennis Ball Detection - tenis_best.pt with tenis_test.mp4\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Model and video file paths\n",
    "model_path = \"basket_best.pt\"\n",
    "video_path = \"basket_test1.mp4\"\n",
    "output_path = \"basket_sonuc.mp4\" # Changed output filename\n",
    "\n",
    "print(f\"Model: {model_path}\")\n",
    "print(f\"Video: {video_path}\")\n",
    "\n",
    "# File checks\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"ERROR: Model file not found: {model_path}\")\n",
    "    exit()\n",
    "\n",
    "if not os.path.exists(video_path):\n",
    "    print(f\"ERROR: Video file not found: {video_path}\")\n",
    "    print(\"Please add tenis_test.mp4 file to the workspace\")\n",
    "    exit()\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading model...\")\n",
    "model = YOLO(model_path)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Open the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f\"Video properties:\")\n",
    "print(f\"  Size: {width}x{height}\")\n",
    "print(f\"  FPS: {fps}\")\n",
    "print(f\"  Total frames: {total_frames}\")\n",
    "\n",
    "# Output video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "print(f\"Processing video...\")\n",
    "\n",
    "frame_count = 0\n",
    "# Removed pothole_detections list as we are focusing on detection for now\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Show progress every 10 frames\n",
    "    if frame_count % 10 == 0:\n",
    "        progress = (frame_count / total_frames) * 100\n",
    "        print(f\"Processed: {frame_count}/{total_frames} frames ({progress:.1f}%)\")\n",
    "\n",
    "    # Perform YOLO detection\n",
    "    results = model(frame, verbose=False)\n",
    "\n",
    "    # Draw results on the frame\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # You can add code here later to process detections for speed calculation\n",
    "    # For now, we will just save the annotated frame\n",
    "\n",
    "    # Write to output video\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"\\nVideo processing finished!\")\n",
    "print(f\"Result video: {output_path}\")\n",
    "\n",
    "# Removed statistics and plotting related to potholes\n",
    "\n",
    "print(f\"\\nCompleted! To open the result video: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da68046b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: basket_best1.pt\n",
      "Video: basket_test.mp4\n",
      "Loading model...\n",
      "Model loaded successfully!\n",
      "Video properties:\n",
      "  Size: 480x852\n",
      "  FPS: 29\n",
      "  Total frames: 256\n",
      "Processing video...\n",
      "Processed: 10/256 frames (3.9%)\n",
      "Processed: 20/256 frames (7.8%)\n",
      "Processed: 30/256 frames (11.7%)\n",
      "Processed: 40/256 frames (15.6%)\n",
      "Processed: 50/256 frames (19.5%)\n",
      "Processed: 60/256 frames (23.4%)\n",
      "Processed: 70/256 frames (27.3%)\n",
      "Processed: 80/256 frames (31.2%)\n",
      "Processed: 90/256 frames (35.2%)\n",
      "Processed: 100/256 frames (39.1%)\n",
      "Processed: 110/256 frames (43.0%)\n",
      "Processed: 120/256 frames (46.9%)\n",
      "Processed: 130/256 frames (50.8%)\n",
      "Processed: 140/256 frames (54.7%)\n",
      "Processed: 150/256 frames (58.6%)\n",
      "Processed: 160/256 frames (62.5%)\n",
      "Processed: 170/256 frames (66.4%)\n",
      "Processed: 180/256 frames (70.3%)\n",
      "Processed: 190/256 frames (74.2%)\n",
      "Processed: 200/256 frames (78.1%)\n",
      "Processed: 210/256 frames (82.0%)\n",
      "Processed: 220/256 frames (85.9%)\n",
      "Processed: 230/256 frames (89.8%)\n",
      "Processed: 240/256 frames (93.8%)\n",
      "Processed: 250/256 frames (97.7%)\n",
      "\n",
      "Video processing finished!\n",
      "Result video: basket_sonuc2.mp4\n",
      "\n",
      "Completed! To open the result video: basket_sonuc2.mp4\n"
     ]
    }
   ],
   "source": [
    "# Video Tennis Ball Detection - tenis_best.pt with tenis_test.mp4\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Model and video file paths\n",
    "model_path = \"basket_best1.pt\"\n",
    "video_path = \"basket_test.mp4\"\n",
    "output_path = \"basket_sonuc2.mp4\" # Changed output filename\n",
    "\n",
    "print(f\"Model: {model_path}\")\n",
    "print(f\"Video: {video_path}\")\n",
    "\n",
    "# File checks\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"ERROR: Model file not found: {model_path}\")\n",
    "    exit()\n",
    "\n",
    "if not os.path.exists(video_path):\n",
    "    print(f\"ERROR: Video file not found: {video_path}\")\n",
    "    print(\"Please add tenis_test.mp4 file to the workspace\")\n",
    "    exit()\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading model...\")\n",
    "model = YOLO(model_path)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Open the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f\"Video properties:\")\n",
    "print(f\"  Size: {width}x{height}\")\n",
    "print(f\"  FPS: {fps}\")\n",
    "print(f\"  Total frames: {total_frames}\")\n",
    "\n",
    "# Output video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "print(f\"Processing video...\")\n",
    "\n",
    "frame_count = 0\n",
    "# Removed pothole_detections list as we are focusing on detection for now\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Show progress every 10 frames\n",
    "    if frame_count % 10 == 0:\n",
    "        progress = (frame_count / total_frames) * 100\n",
    "        print(f\"Processed: {frame_count}/{total_frames} frames ({progress:.1f}%)\")\n",
    "\n",
    "    # Perform YOLO detection\n",
    "    results = model(frame, verbose=False)\n",
    "\n",
    "    # Draw results on the frame\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # You can add code here later to process detections for speed calculation\n",
    "    # For now, we will just save the annotated frame\n",
    "\n",
    "    # Write to output video\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"\\nVideo processing finished!\")\n",
    "print(f\"Result video: {output_path}\")\n",
    "\n",
    "# Removed statistics and plotting related to potholes\n",
    "\n",
    "print(f\"\\nCompleted! To open the result video: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7973589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: basket_best1.pt\n",
      "Video: basket_test2.mp4\n",
      "Loading model...\n",
      "Model loaded successfully!\n",
      "Video properties:\n",
      "  Size: 720x1280\n",
      "  FPS: 30\n",
      "  Total frames: 514\n",
      "Processing video...\n",
      "Processed: 10/514 frames (1.9%)\n",
      "Processed: 20/514 frames (3.9%)\n",
      "Processed: 30/514 frames (5.8%)\n",
      "Processed: 40/514 frames (7.8%)\n",
      "Processed: 50/514 frames (9.7%)\n",
      "Processed: 60/514 frames (11.7%)\n",
      "Processed: 70/514 frames (13.6%)\n",
      "Processed: 80/514 frames (15.6%)\n",
      "Processed: 90/514 frames (17.5%)\n",
      "Processed: 100/514 frames (19.5%)\n",
      "Processed: 110/514 frames (21.4%)\n",
      "Processed: 120/514 frames (23.3%)\n",
      "Processed: 130/514 frames (25.3%)\n",
      "Processed: 140/514 frames (27.2%)\n",
      "Processed: 150/514 frames (29.2%)\n",
      "Processed: 160/514 frames (31.1%)\n",
      "Processed: 170/514 frames (33.1%)\n",
      "Processed: 180/514 frames (35.0%)\n",
      "Processed: 190/514 frames (37.0%)\n",
      "Processed: 200/514 frames (38.9%)\n",
      "Processed: 210/514 frames (40.9%)\n",
      "Processed: 220/514 frames (42.8%)\n",
      "Processed: 230/514 frames (44.7%)\n",
      "Processed: 240/514 frames (46.7%)\n",
      "Processed: 250/514 frames (48.6%)\n",
      "Processed: 260/514 frames (50.6%)\n",
      "Processed: 270/514 frames (52.5%)\n",
      "Processed: 280/514 frames (54.5%)\n",
      "Processed: 290/514 frames (56.4%)\n",
      "Processed: 300/514 frames (58.4%)\n",
      "Processed: 310/514 frames (60.3%)\n",
      "Processed: 320/514 frames (62.3%)\n",
      "Processed: 330/514 frames (64.2%)\n",
      "Processed: 340/514 frames (66.1%)\n",
      "Processed: 350/514 frames (68.1%)\n",
      "Processed: 360/514 frames (70.0%)\n",
      "Processed: 370/514 frames (72.0%)\n",
      "Processed: 380/514 frames (73.9%)\n",
      "Processed: 390/514 frames (75.9%)\n",
      "Processed: 400/514 frames (77.8%)\n",
      "Processed: 410/514 frames (79.8%)\n",
      "Processed: 420/514 frames (81.7%)\n",
      "Processed: 430/514 frames (83.7%)\n",
      "Processed: 440/514 frames (85.6%)\n",
      "Processed: 450/514 frames (87.5%)\n",
      "Processed: 460/514 frames (89.5%)\n",
      "Processed: 470/514 frames (91.4%)\n",
      "Processed: 480/514 frames (93.4%)\n",
      "Processed: 490/514 frames (95.3%)\n",
      "Processed: 500/514 frames (97.3%)\n",
      "Processed: 510/514 frames (99.2%)\n",
      "\n",
      "Video processing finished!\n",
      "Result video: basket_sonuc3.mp4\n",
      "\n",
      "Completed! To open the result video: basket_sonuc3.mp4\n"
     ]
    }
   ],
   "source": [
    "# Video Tennis Ball Detection - tenis_best.pt with tenis_test.mp4\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Model and video file paths\n",
    "model_path = \"basket_best1.pt\"\n",
    "video_path = \"basket_test2.mp4\"\n",
    "output_path = \"basket_sonuc3.mp4\" # Changed output filename\n",
    "\n",
    "print(f\"Model: {model_path}\")\n",
    "print(f\"Video: {video_path}\")\n",
    "\n",
    "# File checks\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"ERROR: Model file not found: {model_path}\")\n",
    "    exit()\n",
    "\n",
    "if not os.path.exists(video_path):\n",
    "    print(f\"ERROR: Video file not found: {video_path}\")\n",
    "    print(\"Please add tenis_test.mp4 file to the workspace\")\n",
    "    exit()\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading model...\")\n",
    "model = YOLO(model_path)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Open the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f\"Video properties:\")\n",
    "print(f\"  Size: {width}x{height}\")\n",
    "print(f\"  FPS: {fps}\")\n",
    "print(f\"  Total frames: {total_frames}\")\n",
    "\n",
    "# Output video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "print(f\"Processing video...\")\n",
    "\n",
    "frame_count = 0\n",
    "# Removed pothole_detections list as we are focusing on detection for now\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Show progress every 10 frames\n",
    "    if frame_count % 10 == 0:\n",
    "        progress = (frame_count / total_frames) * 100\n",
    "        print(f\"Processed: {frame_count}/{total_frames} frames ({progress:.1f}%)\")\n",
    "\n",
    "    # Perform YOLO detection\n",
    "    results = model(frame, verbose=False)\n",
    "\n",
    "    # Draw results on the frame\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # You can add code here later to process detections for speed calculation\n",
    "    # For now, we will just save the annotated frame\n",
    "\n",
    "    # Write to output video\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"\\nVideo processing finished!\")\n",
    "print(f\"Result video: {output_path}\")\n",
    "\n",
    "# Removed statistics and plotting related to potholes\n",
    "\n",
    "print(f\"\\nCompleted! To open the result video: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4f00014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: basket_best.pt\n",
      "Video: basket_test2.mp4\n",
      "Loading model...\n",
      "Model loaded successfully!\n",
      "Video properties:\n",
      "  Size: 720x1280\n",
      "  FPS: 30\n",
      "  Total frames: 514\n",
      "Processing video...\n",
      "Processed: 30/514 frames (5.8%)\n",
      "Processed: 60/514 frames (11.7%)\n",
      "Processed: 90/514 frames (17.5%)\n",
      "Processed: 120/514 frames (23.3%)\n",
      "Processed: 150/514 frames (29.2%)\n",
      "Processed: 180/514 frames (35.0%)\n",
      "Processed: 210/514 frames (40.9%)\n",
      "Processed: 240/514 frames (46.7%)\n",
      "Processed: 270/514 frames (52.5%)\n",
      "Processed: 300/514 frames (58.4%)\n",
      "Processed: 330/514 frames (64.2%)\n",
      "Processed: 360/514 frames (70.0%)\n",
      "Processed: 390/514 frames (75.9%)\n",
      "Processed: 420/514 frames (81.7%)\n",
      "Processed: 450/514 frames (87.5%)\n",
      "Processed: 480/514 frames (93.4%)\n",
      "Processed: 510/514 frames (99.2%)\n",
      "\n",
      "Video processing finished!\n",
      "Result video: basket_sort_sonuc.mp4\n",
      "Total detections logged: 280\n",
      "Final tracked objects: 0\n",
      "Total track histories: 0\n"
     ]
    }
   ],
   "source": [
    "# Video Football Detection with SORT Tracking Algorithm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import deque, defaultdict\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Model and video file paths\n",
    "model_path = \"basket_best.pt\"\n",
    "video_path = \"basket_test2.mp4\"\n",
    "output_path = \"basket_sort_sonuc.mp4\" # SORT tracking output\n",
    "\n",
    "print(f\"Model: {model_path}\")\n",
    "print(f\"Video: {video_path}\")\n",
    "\n",
    "# File checks\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"ERROR: Model file not found: {model_path}\")\n",
    "    exit()\n",
    "\n",
    "if not os.path.exists(video_path):\n",
    "    print(f\"ERROR: Video file not found: {video_path}\")\n",
    "    print(\"Please add futbol_test1.mp4 file to the workspace\")\n",
    "    exit()\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading model...\")\n",
    "model = YOLO(model_path)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Open the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f\"Video properties:\")\n",
    "print(f\"  Size: {width}x{height}\")\n",
    "print(f\"  FPS: {fps}\")\n",
    "print(f\"  Total frames: {total_frames}\")\n",
    "\n",
    "# Output video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "print(f\"Processing video...\")\n",
    "\n",
    "frame_count = 0\n",
    "detection_count = 0\n",
    "\n",
    "# Simple SORT-like Tracker with ID assignment\n",
    "class SimpleTracker:\n",
    "    def __init__(self, max_disappeared=10, max_distance=80):\n",
    "        self.next_id = 1\n",
    "        self.objects = {}  # id -> (center_x, center_y)\n",
    "        self.disappeared = {}  # id -> counter\n",
    "        self.track_histories = defaultdict(lambda: deque(maxlen=20))  # id -> positions\n",
    "        self.speeds = {}  # id -> speed\n",
    "        self.max_disappeared = max_disappeared\n",
    "        self.max_distance = max_distance\n",
    "        \n",
    "    def register(self, center):\n",
    "        \"\"\"Register a new object with new ID\"\"\"\n",
    "        self.objects[self.next_id] = center\n",
    "        self.disappeared[self.next_id] = 0\n",
    "        self.track_histories[self.next_id].append(center)\n",
    "        self.speeds[self.next_id] = 0.0\n",
    "        self.next_id += 1\n",
    "        \n",
    "    def deregister(self, object_id):\n",
    "        \"\"\"Remove an object that has disappeared\"\"\"\n",
    "        del self.objects[object_id]\n",
    "        del self.disappeared[object_id]\n",
    "        if object_id in self.track_histories:\n",
    "            del self.track_histories[object_id]\n",
    "        if object_id in self.speeds:\n",
    "            del self.speeds[object_id]\n",
    "    \n",
    "    def calculate_speed(self, object_id, current_pos):\n",
    "        \"\"\"Calculate speed for an object\"\"\"\n",
    "        if object_id in self.track_histories and len(self.track_histories[object_id]) >= 2:\n",
    "            history = list(self.track_histories[object_id])\n",
    "            if len(history) >= 3:\n",
    "                # Use last 3 positions for stable speed calculation\n",
    "                recent = history[-3:]\n",
    "                dx = recent[-1][0] - recent[0][0]\n",
    "                dy = recent[-1][1] - recent[0][1]\n",
    "                distance = np.sqrt(dx**2 + dy**2)\n",
    "                # 3 positions = 2 frame intervals\n",
    "                speed_per_frame = distance / 2\n",
    "                speed_px_per_sec = speed_per_frame * fps\n",
    "                self.speeds[object_id] = speed_px_per_sec\n",
    "                return speed_px_per_sec\n",
    "        return 0.0\n",
    "        \n",
    "    def update(self, detections):\n",
    "        \"\"\"Update tracker with new detections\"\"\"\n",
    "        # If no existing objects, register all detections\n",
    "        if len(self.objects) == 0:\n",
    "            for detection in detections:\n",
    "                self.register(detection)\n",
    "        else:\n",
    "            # Compute distance matrix between existing objects and detections\n",
    "            object_ids = list(self.objects.keys())\n",
    "            object_centers = list(self.objects.values())\n",
    "            \n",
    "            if len(detections) == 0:\n",
    "                # No detections, mark all as disappeared\n",
    "                for object_id in object_ids:\n",
    "                    self.disappeared[object_id] += 1\n",
    "                    if self.disappeared[object_id] > self.max_disappeared:\n",
    "                        self.deregister(object_id)\n",
    "            else:\n",
    "                # Calculate distances\n",
    "                distances = np.linalg.norm(\n",
    "                    np.array(object_centers)[:, np.newaxis] - np.array(detections), axis=2\n",
    "                )\n",
    "                \n",
    "                # Find the minimum values and sort by distance\n",
    "                rows = distances.min(axis=1).argsort()\n",
    "                cols = distances.argmin(axis=1)[rows]\n",
    "                \n",
    "                # Keep track of used row and column indices\n",
    "                used_rows = set()\n",
    "                used_cols = set()\n",
    "                \n",
    "                # Update existing objects\n",
    "                for (row, col) in zip(rows, cols):\n",
    "                    if row in used_rows or col in used_cols:\n",
    "                        continue\n",
    "                        \n",
    "                    if distances[row, col] <= self.max_distance:\n",
    "                        object_id = object_ids[row]\n",
    "                        self.objects[object_id] = detections[col]\n",
    "                        self.disappeared[object_id] = 0\n",
    "                        self.track_histories[object_id].append(detections[col])\n",
    "                        self.calculate_speed(object_id, detections[col])\n",
    "                        \n",
    "                        used_rows.add(row)\n",
    "                        used_cols.add(col)\n",
    "                \n",
    "                # Handle unmatched detections and objects\n",
    "                unused_rows = set(range(0, distances.shape[0])).difference(used_rows)\n",
    "                unused_cols = set(range(0, distances.shape[1])).difference(used_cols)\n",
    "                \n",
    "                # If more objects than detections, mark objects as disappeared\n",
    "                if distances.shape[0] >= distances.shape[1]:\n",
    "                    for row in unused_rows:\n",
    "                        object_id = object_ids[row]\n",
    "                        self.disappeared[object_id] += 1\n",
    "                        \n",
    "                        if self.disappeared[object_id] > self.max_disappeared:\n",
    "                            self.deregister(object_id)\n",
    "                \n",
    "                # If more detections than objects, register new objects\n",
    "                else:\n",
    "                    for col in unused_cols:\n",
    "                        self.register(detections[col])\n",
    "        \n",
    "        return self.objects, self.track_histories, self.speeds\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = SimpleTracker(max_disappeared=15, max_distance=100)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Show progress every 30 frames\n",
    "    if frame_count % 30 == 0:\n",
    "        progress = (frame_count / total_frames) * 100\n",
    "        print(f\"Processed: {frame_count}/{total_frames} frames ({progress:.1f}%)\")\n",
    "\n",
    "    # Perform YOLO detection\n",
    "    results = model(frame, verbose=False)\n",
    "\n",
    "    # Get the boxes\n",
    "    boxes = results[0].boxes.xywh.cpu() if results[0].boxes is not None else []\n",
    "\n",
    "    # Create a copy of the frame to draw on\n",
    "    annotated_frame = frame.copy()\n",
    "    \n",
    "    # Prepare detections for tracker\n",
    "    detections = []\n",
    "    if len(boxes) > 0:\n",
    "        detection_count += 1\n",
    "        \n",
    "        for box in boxes:\n",
    "            x, y, w, h = box\n",
    "            detections.append((float(x), float(y)))\n",
    "            \n",
    "            # Draw detection bounding box\n",
    "            x1, y1, x2, y2 = int(x - w/2), int(y - h/2), int(x + w/2), int(y + h/2)\n",
    "            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2) # Green detection box\n",
    "\n",
    "    # Update tracker\n",
    "    tracked_objects, track_histories, speeds = tracker.update(detections)\n",
    "    \n",
    "    # Draw tracking results\n",
    "    for object_id, (x, y) in tracked_objects.items():\n",
    "        # Draw tracked object\n",
    "        cv2.circle(annotated_frame, (int(x), int(y)), 10, (255, 0, 0), -1) # Blue tracked point\n",
    "        \n",
    "        # Draw ID\n",
    "        cv2.putText(annotated_frame, f\"ID:{object_id}\", (int(x) + 15, int(y) - 15),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "        \n",
    "        # Draw track history\n",
    "        if object_id in track_histories and len(track_histories[object_id]) > 1:\n",
    "            points = np.array([(int(p[0]), int(p[1])) for p in track_histories[object_id]], dtype=np.int32)\n",
    "            cv2.polylines(annotated_frame, [points.reshape((-1, 1, 2))], False, (255, 0, 255), 3) # Magenta track\n",
    "            \n",
    "            # Draw track points with fading effect\n",
    "            for i, point in enumerate(track_histories[object_id]):\n",
    "                alpha = (i + 1) / len(track_histories[object_id])\n",
    "                radius = int(3 + alpha * 2)\n",
    "                cv2.circle(annotated_frame, (int(point[0]), int(point[1])), radius, (255, 0, 255), -1)\n",
    "        \n",
    "        # Draw speed vector if moving\n",
    "        if object_id in speeds and speeds[object_id] > 5:\n",
    "            if len(track_histories[object_id]) >= 2:\n",
    "                current_pos = track_histories[object_id][-1]\n",
    "                prev_pos = track_histories[object_id][-2]\n",
    "                dx = current_pos[0] - prev_pos[0]\n",
    "                dy = current_pos[1] - prev_pos[1]\n",
    "                \n",
    "                if abs(dx) > 0.1 or abs(dy) > 0.1:\n",
    "                    scale = min(speeds[object_id] / 10, 40)\n",
    "                    norm = np.sqrt(dx**2 + dy**2)\n",
    "                    dx_norm = (dx / norm) * scale\n",
    "                    dy_norm = (dy / norm) * scale\n",
    "                    \n",
    "                    end_x = int(x + dx_norm)\n",
    "                    end_y = int(y + dy_norm)\n",
    "                    cv2.arrowedLine(annotated_frame, (int(x), int(y)), (end_x, end_y), (0, 0, 255), 3)\n",
    "\n",
    "    # Information panel\n",
    "    overlay = annotated_frame.copy()\n",
    "    cv2.rectangle(overlay, (10, 10), (520, 195), (0, 0, 0), -1)\n",
    "    cv2.addWeighted(overlay, 0.8, annotated_frame, 0.2, 0, annotated_frame)\n",
    "    \n",
    "    # Display info\n",
    "    cv2.putText(annotated_frame, f\"Algoritma    : SORT TRACKER\", (20, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2) # Yellow\n",
    "    \n",
    "    # Show speeds for all tracked objects\n",
    "    if tracked_objects:\n",
    "        active_ids = list(tracked_objects.keys())\n",
    "        speed_info = \", \".join([f\"ID{id}:{speeds.get(id, 0):.0f}\" for id in active_ids])\n",
    "        cv2.putText(annotated_frame, f\"Hizlar       : {speed_info} px/s\", (20, 65),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2) # Orange\n",
    "    else:\n",
    "        cv2.putText(annotated_frame, f\"Hizlar       : BEKLIYOR\", (20, 65),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2) # Orange\n",
    "    \n",
    "    status = f\"AKTIF ({len(tracked_objects)} obje)\" if tracked_objects else \"BEKLENIYOR\"\n",
    "    cv2.putText(annotated_frame, f\"Durum        : {status}\", (20, 90),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2) # Green\n",
    "    \n",
    "    cv2.putText(annotated_frame, f\"Frame        : {frame_count}\", (20, 115),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) # White\n",
    "    \n",
    "    detection_text = f\"{len(detections)}\" if detections else \"0\"\n",
    "    cv2.putText(annotated_frame, f\"Tespit       : {detection_text} obje\", (20, 140),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) # White\n",
    "    \n",
    "    total_track_points = sum(len(history) for history in track_histories.values())\n",
    "    cv2.putText(annotated_frame, f\"Takip        : {total_track_points} nokta\", (20, 165),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) # White\n",
    "    \n",
    "    cv2.putText(annotated_frame, f\"Aktif ID     : {list(tracked_objects.keys())}\", (20, 190),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) # White\n",
    "\n",
    "    # Write to output video\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"\\nVideo processing finished!\")\n",
    "print(f\"Result video: {output_path}\")\n",
    "print(f\"Total detections logged: {detection_count}\")\n",
    "print(f\"Final tracked objects: {len(tracker.objects)}\")\n",
    "print(f\"Total track histories: {len(tracker.track_histories)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb6a748d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: basket_best1.pt\n",
      "Video: basket_test2.mp4\n",
      "Loading model...\n",
      "Model loaded successfully!\n",
      "Video properties:\n",
      "  Size: 720x1280\n",
      "  FPS: 30\n",
      "  Total frames: 514\n",
      "Processing video with DeepSORT...\n",
      "Processed: 30/514 frames (5.8%)\n",
      "Processed: 60/514 frames (11.7%)\n",
      "Processed: 90/514 frames (17.5%)\n",
      "Processed: 120/514 frames (23.3%)\n",
      "Processed: 150/514 frames (29.2%)\n",
      "Processed: 180/514 frames (35.0%)\n",
      "Processed: 210/514 frames (40.9%)\n",
      "Processed: 240/514 frames (46.7%)\n",
      "Processed: 270/514 frames (52.5%)\n",
      "Processed: 300/514 frames (58.4%)\n",
      "Processed: 330/514 frames (64.2%)\n",
      "Processed: 360/514 frames (70.0%)\n",
      "Processed: 390/514 frames (75.9%)\n",
      "Processed: 420/514 frames (81.7%)\n",
      "Processed: 450/514 frames (87.5%)\n",
      "Processed: 480/514 frames (93.4%)\n",
      "Processed: 510/514 frames (99.2%)\n",
      "\n",
      "Video processing finished!\n",
      "Result video: bs_deepsort_sonuc.mp4\n",
      "Total detections logged: 318\n",
      "Total tracks created: 17\n",
      "Max simultaneous tracks: 2\n",
      "Highest speed recorded: 1807.8 px/s\n"
     ]
    }
   ],
   "source": [
    "# Video Football Detection with DeepSORT Tracking\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import deque, defaultdict\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Model and video file paths\n",
    "model_path = \"basket_best1.pt\"\n",
    "video_path = \"basket_test2.mp4\"  # Fixed: double dots in filename\n",
    "output_path = \"bs_deepsort_sonuc.mp4\"\n",
    "\n",
    "print(f\"Model: {model_path}\")\n",
    "print(f\"Video: {video_path}\")\n",
    "\n",
    "# File checks\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"ERROR: Model file not found: {model_path}\")\n",
    "    exit()\n",
    "\n",
    "if not os.path.exists(video_path):\n",
    "    print(f\"ERROR: Video file not found: {video_path}\")\n",
    "    print(\"Please add futbol_test2..mp4 file to the workspace\")\n",
    "    exit()\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading model...\")\n",
    "model = YOLO(model_path)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Open the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f\"Video properties:\")\n",
    "print(f\"  Size: {width}x{height}\")\n",
    "print(f\"  FPS: {fps}\")\n",
    "print(f\"  Total frames: {total_frames}\")\n",
    "\n",
    "# Output video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "print(f\"Processing video with DeepSORT...\")\n",
    "\n",
    "frame_count = 0\n",
    "detection_count = 0\n",
    "\n",
    "# DeepSORT Tracker Implementation\n",
    "class DeepSORTTracker:\n",
    "    def __init__(self, max_disappeared=15, max_distance=100):\n",
    "        self.next_id = 0\n",
    "        self.tracks = {}  # {id: {'position': (x,y), 'history': deque, 'age': int, 'hits': int, 'disappeared': int}}\n",
    "        self.max_disappeared = max_disappeared\n",
    "        self.max_distance = max_distance\n",
    "        self.colors = {}\n",
    "        self.speed_history = defaultdict(lambda: deque(maxlen=5))\n",
    "        \n",
    "        # Feature extraction for deep association (simplified)\n",
    "        self.feature_history = defaultdict(lambda: deque(maxlen=5))\n",
    "        \n",
    "    def extract_features(self, frame, bbox):\n",
    "        \"\"\"Extract simple features from bounding box area (simplified DeepSORT feature extraction)\"\"\"\n",
    "        x, y, w, h = bbox\n",
    "        x1, y1, x2, y2 = int(x - w/2), int(y - h/2), int(x + w/2), int(y + h/2)\n",
    "        \n",
    "        # Ensure coordinates are within frame bounds\n",
    "        x1 = max(0, min(x1, width-1))\n",
    "        y1 = max(0, min(y1, height-1))\n",
    "        x2 = max(0, min(x2, width-1))\n",
    "        y2 = max(0, min(y2, height-1))\n",
    "        \n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            return np.zeros(32)  # Return zero features if invalid bbox\n",
    "        \n",
    "        # Extract patch and compute simple features\n",
    "        patch = frame[y1:y2, x1:x2]\n",
    "        \n",
    "        if patch.size == 0:\n",
    "            return np.zeros(32)\n",
    "        \n",
    "        # Resize patch to standard size\n",
    "        try:\n",
    "            patch_resized = cv2.resize(patch, (32, 32))\n",
    "            \n",
    "            # Compute histogram features (RGB channels)\n",
    "            features = []\n",
    "            for i in range(3):  # For each color channel\n",
    "                hist = cv2.calcHist([patch_resized], [i], None, [8], [0, 256])\n",
    "                features.extend(hist.flatten())\n",
    "            \n",
    "            # Add texture features (gradient magnitude)\n",
    "            gray_patch = cv2.cvtColor(patch_resized, cv2.COLOR_BGR2GRAY)\n",
    "            grad_x = cv2.Sobel(gray_patch, cv2.CV_64F, 1, 0, ksize=3)\n",
    "            grad_y = cv2.Sobel(gray_patch, cv2.CV_64F, 0, 1, ksize=3)\n",
    "            grad_mag = np.sqrt(grad_x**2 + grad_y**2)\n",
    "            texture_hist = np.histogram(grad_mag, bins=8)[0]\n",
    "            features.extend(texture_hist)\n",
    "            \n",
    "            return np.array(features, dtype=np.float32)\n",
    "        except:\n",
    "            return np.zeros(32)\n",
    "    \n",
    "    def compute_feature_similarity(self, feat1, feat2):\n",
    "        \"\"\"Compute cosine similarity between features\"\"\"\n",
    "        if np.linalg.norm(feat1) == 0 or np.linalg.norm(feat2) == 0:\n",
    "            return 0.0\n",
    "        return np.dot(feat1, feat2) / (np.linalg.norm(feat1) * np.linalg.norm(feat2))\n",
    "    \n",
    "    def get_color(self, track_id):\n",
    "        \"\"\"Get consistent color for track ID\"\"\"\n",
    "        if track_id not in self.colors:\n",
    "            np.random.seed(track_id)\n",
    "            self.colors[track_id] = (\n",
    "                np.random.randint(50, 255),\n",
    "                np.random.randint(50, 255),\n",
    "                np.random.randint(50, 255)\n",
    "            )\n",
    "        return self.colors[track_id]\n",
    "    \n",
    "    def update(self, frame, detections):\n",
    "        \"\"\"Update tracker with new detections\"\"\"\n",
    "        if len(detections) == 0:\n",
    "            # Age existing tracks\n",
    "            tracks_to_delete = []\n",
    "            for track_id in self.tracks:\n",
    "                self.tracks[track_id]['disappeared'] += 1\n",
    "                if self.tracks[track_id]['disappeared'] > self.max_disappeared:\n",
    "                    tracks_to_delete.append(track_id)\n",
    "            \n",
    "            for track_id in tracks_to_delete:\n",
    "                del self.tracks[track_id]\n",
    "            \n",
    "            return []\n",
    "        \n",
    "        # Extract features for all detections\n",
    "        detection_features = []\n",
    "        for det in detections:\n",
    "            feat = self.extract_features(frame, det)\n",
    "            detection_features.append(feat)\n",
    "        \n",
    "        if len(self.tracks) == 0:\n",
    "            # Initialize new tracks\n",
    "            for i, detection in enumerate(detections):\n",
    "                x, y, w, h = detection\n",
    "                self.tracks[self.next_id] = {\n",
    "                    'position': (float(x), float(y)),\n",
    "                    'bbox': detection,\n",
    "                    'history': deque(maxlen=20),\n",
    "                    'age': 1,\n",
    "                    'hits': 1,\n",
    "                    'disappeared': 0\n",
    "                }\n",
    "                self.tracks[self.next_id]['history'].append((float(x), float(y)))\n",
    "                self.feature_history[self.next_id].append(detection_features[i])\n",
    "                self.next_id += 1\n",
    "        else:\n",
    "            # Match detections to existing tracks using Hungarian algorithm\n",
    "            track_ids = list(self.tracks.keys())\n",
    "            \n",
    "            # Compute cost matrix (distance + feature similarity)\n",
    "            cost_matrix = np.zeros((len(track_ids), len(detections)))\n",
    "            \n",
    "            for i, track_id in enumerate(track_ids):\n",
    "                track_pos = self.tracks[track_id]['position']\n",
    "                track_features = self.feature_history[track_id]\n",
    "                avg_track_features = np.mean(track_features, axis=0) if len(track_features) > 0 else np.zeros(32)\n",
    "                \n",
    "                for j, detection in enumerate(detections):\n",
    "                    x, y, w, h = detection\n",
    "                    det_pos = (float(x), float(y))\n",
    "                    \n",
    "                    # Spatial distance\n",
    "                    spatial_dist = euclidean(track_pos, det_pos)\n",
    "                    \n",
    "                    # Feature similarity (convert to distance)\n",
    "                    feature_sim = self.compute_feature_similarity(avg_track_features, detection_features[j])\n",
    "                    feature_dist = 1.0 - feature_sim\n",
    "                    \n",
    "                    # Combined cost (weighted sum)\n",
    "                    cost_matrix[i, j] = 0.7 * spatial_dist + 0.3 * feature_dist * 100\n",
    "            \n",
    "            # Solve assignment problem\n",
    "            if cost_matrix.size > 0:\n",
    "                row_indices, col_indices = linear_sum_assignment(cost_matrix)\n",
    "                \n",
    "                # Update matched tracks\n",
    "                matched_tracks = set()\n",
    "                matched_detections = set()\n",
    "                \n",
    "                for row_idx, col_idx in zip(row_indices, col_indices):\n",
    "                    if cost_matrix[row_idx, col_idx] < self.max_distance:\n",
    "                        track_id = track_ids[row_idx]\n",
    "                        detection = detections[col_idx]\n",
    "                        x, y, w, h = detection\n",
    "                        \n",
    "                        # Update track\n",
    "                        self.tracks[track_id]['position'] = (float(x), float(y))\n",
    "                        self.tracks[track_id]['bbox'] = detection\n",
    "                        self.tracks[track_id]['history'].append((float(x), float(y)))\n",
    "                        self.tracks[track_id]['age'] += 1\n",
    "                        self.tracks[track_id]['hits'] += 1\n",
    "                        self.tracks[track_id]['disappeared'] = 0\n",
    "                        self.feature_history[track_id].append(detection_features[col_idx])\n",
    "                        \n",
    "                        matched_tracks.add(track_id)\n",
    "                        matched_detections.add(col_idx)\n",
    "                \n",
    "                # Handle unmatched tracks\n",
    "                for track_id in track_ids:\n",
    "                    if track_id not in matched_tracks:\n",
    "                        self.tracks[track_id]['disappeared'] += 1\n",
    "                \n",
    "                # Handle unmatched detections (create new tracks)\n",
    "                for j, detection in enumerate(detections):\n",
    "                    if j not in matched_detections:\n",
    "                        x, y, w, h = detection\n",
    "                        self.tracks[self.next_id] = {\n",
    "                            'position': (float(x), float(y)),\n",
    "                            'bbox': detection,\n",
    "                            'history': deque(maxlen=20),\n",
    "                            'age': 1,\n",
    "                            'hits': 1,\n",
    "                            'disappeared': 0\n",
    "                        }\n",
    "                        self.tracks[self.next_id]['history'].append((float(x), float(y)))\n",
    "                        self.feature_history[self.next_id].append(detection_features[j])\n",
    "                        self.next_id += 1\n",
    "                \n",
    "                # Remove disappeared tracks\n",
    "                tracks_to_delete = []\n",
    "                for track_id in self.tracks:\n",
    "                    if self.tracks[track_id]['disappeared'] > self.max_disappeared:\n",
    "                        tracks_to_delete.append(track_id)\n",
    "                \n",
    "                for track_id in tracks_to_delete:\n",
    "                    del self.tracks[track_id]\n",
    "                    if track_id in self.feature_history:\n",
    "                        del self.feature_history[track_id]\n",
    "        \n",
    "        # Return active tracks (only confirmed tracks)\n",
    "        active_tracks = []\n",
    "        for track_id, track in self.tracks.items():\n",
    "            if track['hits'] >= 3 and track['disappeared'] < 5:  # Confirmed tracks\n",
    "                active_tracks.append((track_id, track))\n",
    "        \n",
    "        return active_tracks\n",
    "    \n",
    "    def get_speed(self, track_id):\n",
    "        \"\"\"Calculate speed for a track\"\"\"\n",
    "        if track_id not in self.tracks or len(self.tracks[track_id]['history']) < 3:\n",
    "            return 0.0\n",
    "        \n",
    "        history = list(self.tracks[track_id]['history'])\n",
    "        recent_positions = history[-3:]\n",
    "        \n",
    "        dx = recent_positions[-1][0] - recent_positions[0][0]\n",
    "        dy = recent_positions[-1][1] - recent_positions[0][1]\n",
    "        \n",
    "        distance_pixels = np.sqrt(dx**2 + dy**2)\n",
    "        frames_interval = 2\n",
    "        speed_per_frame = distance_pixels / frames_interval\n",
    "        speed_px_per_sec = speed_per_frame * fps\n",
    "        \n",
    "        # Store speed history for smoothing\n",
    "        self.speed_history[track_id].append(speed_px_per_sec)\n",
    "        \n",
    "        # Return smoothed speed\n",
    "        return np.mean(self.speed_history[track_id])\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "tracker = DeepSORTTracker(max_disappeared=20, max_distance=80)\n",
    "\n",
    "# Statistics\n",
    "total_tracks_created = 0\n",
    "max_simultaneous_tracks = 0\n",
    "speed_records = {}\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Show progress every 30 frames\n",
    "    if frame_count % 30 == 0:\n",
    "        progress = (frame_count / total_frames) * 100\n",
    "        print(f\"Processed: {frame_count}/{total_frames} frames ({progress:.1f}%)\")\n",
    "\n",
    "    # Perform YOLO detection\n",
    "    results = model(frame, verbose=False)\n",
    "\n",
    "    # Get the boxes\n",
    "    boxes = results[0].boxes.xywh.cpu() if results[0].boxes is not None else []\n",
    "\n",
    "    # Create a copy of the frame to draw on\n",
    "    annotated_frame = frame.copy()\n",
    "\n",
    "    # Prepare detections for tracker\n",
    "    detections = []\n",
    "    if len(boxes) > 0:\n",
    "        detection_count += 1\n",
    "        for box in boxes:\n",
    "            x, y, w, h = box\n",
    "            detections.append((float(x), float(y), float(w), float(h)))\n",
    "\n",
    "    # Update tracker\n",
    "    active_tracks = tracker.update(frame, detections)\n",
    "    \n",
    "    # Update statistics\n",
    "    current_track_count = len(active_tracks)\n",
    "    max_simultaneous_tracks = max(max_simultaneous_tracks, current_track_count)\n",
    "    if tracker.next_id > total_tracks_created:\n",
    "        total_tracks_created = tracker.next_id\n",
    "\n",
    "    # Draw tracking results\n",
    "    for track_id, track in active_tracks:\n",
    "        x, y = track['position']\n",
    "        bbox = track['bbox']\n",
    "        color = tracker.get_color(track_id)\n",
    "        \n",
    "        # Draw bounding box\n",
    "        bx, by, bw, bh = bbox\n",
    "        x1, y1, x2, y2 = int(bx - bw/2), int(by - bh/2), int(bx + bw/2), int(by + bh/2)\n",
    "        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 3)\n",
    "        \n",
    "        # Draw ID label\n",
    "        cv2.putText(annotated_frame, f\"ID:{track_id}\", (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "        \n",
    "        # Draw track history\n",
    "        if len(track['history']) > 1:\n",
    "            points = np.array([(int(p[0]), int(p[1])) for p in track['history']], dtype=np.int32)\n",
    "            cv2.polylines(annotated_frame, [points.reshape((-1, 1, 2))], False, color, 2)\n",
    "            \n",
    "            # Draw track points with fading effect\n",
    "            for i, point in enumerate(track['history']):\n",
    "                alpha = (i + 1) / len(track['history'])\n",
    "                radius = int(3 + alpha * 2)\n",
    "                cv2.circle(annotated_frame, (int(point[0]), int(point[1])), radius, color, -1)\n",
    "        \n",
    "        # Calculate and display speed\n",
    "        speed = tracker.get_speed(track_id)\n",
    "        if speed > 0:\n",
    "            speed_records[track_id] = max(speed_records.get(track_id, 0), speed)\n",
    "            cv2.putText(annotated_frame, f\"{speed:.1f} px/s\", (x1, y2 + 25),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    # Information panel\n",
    "    overlay = annotated_frame.copy()\n",
    "    cv2.rectangle(overlay, (10, 10), (580, 195), (0, 0, 0), -1)\n",
    "    cv2.addWeighted(overlay, 0.8, annotated_frame, 0.2, 0, annotated_frame)\n",
    "    \n",
    "    # Display info\n",
    "    cv2.putText(annotated_frame, f\"Algoritma         : DEEPSORT\", (20, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)  # Yellow\n",
    "    \n",
    "    cv2.putText(annotated_frame, f\"Aktif Takip       : {current_track_count} obje\", (20, 65),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)  # Green\n",
    "    \n",
    "    cv2.putText(annotated_frame, f\"Toplam ID         : {total_tracks_created}\", (20, 90),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)  # Orange\n",
    "    \n",
    "    cv2.putText(annotated_frame, f\"Max Esmanli       : {max_simultaneous_tracks}\", (20, 115),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)  # Magenta\n",
    "    \n",
    "    cv2.putText(annotated_frame, f\"Frame             : {frame_count}\", (20, 140),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)  # White\n",
    "    \n",
    "    detection_text = f\"{len(boxes)}\" if len(boxes) > 0 else \"0\"\n",
    "    cv2.putText(annotated_frame, f\"Tespit            : {detection_text} obje\", (20, 165),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)  # White\n",
    "    \n",
    "    # Max speed info\n",
    "    max_speed = max(speed_records.values()) if speed_records else 0\n",
    "    cv2.putText(annotated_frame, f\"Max Hiz           : {max_speed:.1f} px/s\", (20, 190),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)  # Cyan\n",
    "\n",
    "    # Write to output video\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"\\nVideo processing finished!\")\n",
    "print(f\"Result video: {output_path}\")\n",
    "print(f\"Total detections logged: {detection_count}\")\n",
    "print(f\"Total tracks created: {total_tracks_created}\")\n",
    "print(f\"Max simultaneous tracks: {max_simultaneous_tracks}\")\n",
    "if speed_records:\n",
    "    print(f\"Highest speed recorded: {max(speed_records.values()):.1f} px/s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
